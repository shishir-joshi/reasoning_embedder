{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f03937e",
   "metadata": {},
   "source": [
    "# Explore Prepared Dataset (Minimal)\n",
    "\n",
    "Loads prepared dataset samples, builds a tiny cosine-sim index using Qwen 0.6B embeddings, and lets you run a quick text query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad331763-65a5-4054-bc83-da680ad0662d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/shishirjoshi/development/lab/reasoning_embedder\n"
     ]
    }
   ],
   "source": [
    "%cd ~/development/lab/reasoning_embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3394275d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shishirjoshi/development/lab/reasoning_embedder/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Minimal imports\n",
    "import os, json, glob, math\n",
    "from typing import List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Try sentence-transformers first, fall back to transformers if needed\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    _USE_ST = True\n",
    "except Exception:\n",
    "    from transformers import AutoTokenizer, AutoModel\n",
    "    import torch\n",
    "    _USE_ST = False\n",
    "\n",
    "DATA_DIR = 'data/prepared_reasonir_hq'  # change if your prepared set lives elsewhere\n",
    "MODEL_NAME = 'Qwen/Qwen3-Embedding-0.6B'\n",
    "DEVICE = 'cuda' if (not _USE_ST and 'torch' in globals() and torch.cuda.is_available()) else ('mps' if (not _USE_ST and 'torch' in globals() and hasattr(torch.backends, 'mps') and torch.backends.mps.is_available()) else 'cpu')\n",
    "MAX_DOCS = 300  # keep minimal for a quick demo index\n",
    "TOP_K = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc8e1b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to read HF dataset at data/prepared_reasonir_hq: Feature type 'List' not found. Available feature types: ['Value', 'ClassLabel', 'Translation', 'TranslationVariableLanguages', 'LargeList', 'Sequence', 'Array2D', 'Array3D', 'Array4D', 'Array5D', 'Audio', 'Image', 'Video', 'Pdf']\n",
      "No HF dataset or JSONL files found in data/prepared_reasonir_hq. Ensure you have run `reason-prepare`.\n",
      "Loaded 0 docs for the mini index.\n"
     ]
    }
   ],
   "source": [
    "# Minimal loader: support HF dataset saved to disk OR fallback JSONL scan\n",
    "def _extract_texts_from_json(obj) -> List[str]:\n",
    "    texts = []\n",
    "    if isinstance(obj, dict):\n",
    "        for key in ('document_text', 'text', 'doc', 'content'):\n",
    "            if key in obj and isinstance(obj[key], str):\n",
    "                texts.append(obj[key])\n",
    "        for key in ('pos', 'neg', 'positives', 'negatives', 'pairs'):\n",
    "            if key in obj and isinstance(obj[key], list):\n",
    "                for item in obj[key]:\n",
    "                    if isinstance(item, list) and len(item) == 2 and isinstance(item[1], str):\n",
    "                        texts.append(item[1])\n",
    "    elif isinstance(obj, list):\n",
    "        if len(obj) == 2 and isinstance(obj[1], str):\n",
    "            texts.append(obj[1])\n",
    "        else:\n",
    "            for it in obj:\n",
    "                if isinstance(it, str):\n",
    "                    texts.append(it)\n",
    "    return texts\n",
    "\n",
    "def load_corpus_from_prepared(data_dir: str, max_docs: int = 300) -> List[str]:\n",
    "    corpus: List[str] = []\n",
    "    ds_path = os.path.join(data_dir, 'dataset_dict.json')\n",
    "    if os.path.exists(ds_path):\n",
    "        # Try HuggingFace load_from_disk first\n",
    "        try:\n",
    "            from datasets import load_from_disk\n",
    "            dsd = load_from_disk(data_dir)  # DatasetDict with splits like 'train'\n",
    "            split = dsd['train'] if 'train' in dsd else list(dsd.values())[0]\n",
    "            for ex in split.select(range(min(max_docs, len(split)))):\n",
    "                if isinstance(ex, dict):\n",
    "                    if isinstance(ex.get('document_text'), str):\n",
    "                        corpus.append(ex['document_text'])\n",
    "                        continue\n",
    "                    for key in ('pos', 'neg', 'positives', 'negatives', 'pairs'):\n",
    "                        val = ex.get(key)\n",
    "                        if isinstance(val, list) and val:\n",
    "                            itm = val[0]\n",
    "                            if isinstance(itm, list) and len(itm) == 2 and isinstance(itm[1], str):\n",
    "                                corpus.append(itm[1])\n",
    "                                break\n",
    "                if len(corpus) < max_docs and isinstance(ex, dict):\n",
    "                    for key in ('text', 'doc', 'content'):\n",
    "                        if isinstance(ex.get(key), str):\n",
    "                            corpus.append(ex[key])\n",
    "                            break\n",
    "            return corpus\n",
    "        except Exception as e:\n",
    "            print(f'Failed to read HF dataset at {data_dir}: {e}')\n",
    "            # Fallback to reading Arrow directly via pyarrow\n",
    "            try:\n",
    "                import pyarrow as pa, pyarrow.ipc as pa_ipc\n",
    "                arrow_files = glob.glob(os.path.join(data_dir, 'train', '**', '*.arrow'), recursive=True)\n",
    "                if not arrow_files:\n",
    "                    arrow_files = glob.glob(os.path.join(data_dir, 'train', '*.arrow'))\n",
    "                taken = 0\n",
    "                for ap in arrow_files:\n",
    "                    with pa.memory_map(ap, 'r') as source:\n",
    "                        reader = pa_ipc.open_file(source)\n",
    "                        table = reader.read_all()\n",
    "                    rows = table.to_pylist()\n",
    "                    for row in rows:\n",
    "                        if taken >= max_docs:\n",
    "                            return corpus\n",
    "                        # Reuse JSON extractor on the row dict\n",
    "                        texts = _extract_texts_from_json(row)\n",
    "                        if texts:\n",
    "                            corpus.append(texts[0])\n",
    "                            taken += 1\n",
    "                if corpus:\n",
    "                    return corpus\n",
    "            except Exception as e2:\n",
    "                print(f'Failed to read Arrow directly at {data_dir}: {e2}')\n",
    "            # fall through to JSONL scan\n",
    "    # JSONL fallback\n",
    "    jsonl_paths = glob.glob(os.path.join(data_dir, '**', '*.jsonl'), recursive=True)\n",
    "    if not jsonl_paths:\n",
    "        print(f'No HF dataset or JSONL files found in {data_dir}. Ensure you have run `reason-prepare`.')\n",
    "        return corpus\n",
    "    for p in jsonl_paths:\n",
    "        with open(p, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                try:\n",
    "                    obj = json.loads(line)\n",
    "                except Exception:\n",
    "                    continue\n",
    "                texts = _extract_texts_from_json(obj)\n",
    "                for t in texts:\n",
    "                    if t and isinstance(t, str):\n",
    "                        corpus.append(t)\n",
    "                        if len(corpus) >= max_docs:\n",
    "                            return corpus\n",
    "    return corpus\n",
    "\n",
    "corpus_texts = load_corpus_from_prepared(DATA_DIR, MAX_DOCS)\n",
    "print(f'Loaded {len(corpus_texts)} docs for the mini index.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "644faa07",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The checkpoint you are trying to load has model type `qwen3` but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date.\n\nYou can update Transformers with the command `pip install --upgrade transformers`. If this does not work, and the checkpoint is very new, then there may not be a release version that supports this model yet. In this case, you can get the most up-to-date code by installing Transformers from source with the command `pip install git+https://github.com/huggingface/transformers.git`",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/development/lab/reasoning_embedder/venv/lib/python3.12/site-packages/transformers/models/auto/configuration_auto.py:1071\u001b[39m, in \u001b[36mAutoConfig.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[39m\n\u001b[32m   1070\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1071\u001b[39m     config_class = \u001b[43mCONFIG_MAPPING\u001b[49m\u001b[43m[\u001b[49m\u001b[43mconfig_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel_type\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m   1072\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/development/lab/reasoning_embedder/venv/lib/python3.12/site-packages/transformers/models/auto/configuration_auto.py:773\u001b[39m, in \u001b[36m_LazyConfigMapping.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m    772\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._mapping:\n\u001b[32m--> \u001b[39m\u001b[32m773\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[32m    774\u001b[39m value = \u001b[38;5;28mself\u001b[39m._mapping[key]\n",
      "\u001b[31mKeyError\u001b[39m: 'qwen3'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Load embedding model (minimal)\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _USE_ST:\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     model = \u001b[43mSentenceTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMODEL_NAME\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mencode_texts\u001b[39m(texts: List[\u001b[38;5;28mstr\u001b[39m]) -> np.ndarray:\n\u001b[32m      5\u001b[39m         embs = model.encode(texts, convert_to_numpy=\u001b[38;5;28;01mTrue\u001b[39;00m, show_progress_bar=\u001b[38;5;28;01mFalse\u001b[39;00m, normalize_embeddings=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/development/lab/reasoning_embedder/venv/lib/python3.12/site-packages/sentence_transformers/SentenceTransformer.py:309\u001b[39m, in \u001b[36mSentenceTransformer.__init__\u001b[39m\u001b[34m(self, model_name_or_path, modules, device, prompts, default_prompt_name, similarity_fn_name, cache_folder, trust_remote_code, revision, local_files_only, token, use_auth_token, truncate_dim, model_kwargs, tokenizer_kwargs, config_kwargs, model_card_data, backend)\u001b[39m\n\u001b[32m    300\u001b[39m         model_name_or_path = __MODEL_HUB_ORGANIZATION__ + \u001b[33m\"\u001b[39m\u001b[33m/\u001b[39m\u001b[33m\"\u001b[39m + model_name_or_path\n\u001b[32m    302\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentence_transformer_model(\n\u001b[32m    303\u001b[39m     model_name_or_path,\n\u001b[32m    304\u001b[39m     token,\n\u001b[32m   (...)\u001b[39m\u001b[32m    307\u001b[39m     local_files_only=local_files_only,\n\u001b[32m    308\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m309\u001b[39m     modules, \u001b[38;5;28mself\u001b[39m.module_kwargs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_sbert_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    310\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    311\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    312\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    313\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    314\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    315\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    318\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    319\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    320\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    321\u001b[39m     modules = \u001b[38;5;28mself\u001b[39m._load_auto_model(\n\u001b[32m    322\u001b[39m         model_name_or_path,\n\u001b[32m    323\u001b[39m         token=token,\n\u001b[32m   (...)\u001b[39m\u001b[32m    330\u001b[39m         config_kwargs=config_kwargs,\n\u001b[32m    331\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/development/lab/reasoning_embedder/venv/lib/python3.12/site-packages/sentence_transformers/SentenceTransformer.py:1802\u001b[39m, in \u001b[36mSentenceTransformer._load_sbert_model\u001b[39m\u001b[34m(self, model_name_or_path, token, cache_folder, revision, trust_remote_code, local_files_only, model_kwargs, tokenizer_kwargs, config_kwargs)\u001b[39m\n\u001b[32m   1799\u001b[39m \u001b[38;5;66;03m# Try to initialize the module with a lot of kwargs, but only if the module supports them\u001b[39;00m\n\u001b[32m   1800\u001b[39m \u001b[38;5;66;03m# Otherwise we fall back to the load method\u001b[39;00m\n\u001b[32m   1801\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1802\u001b[39m     module = \u001b[43mmodule_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1803\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   1804\u001b[39m     module = module_class.load(model_name_or_path)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/development/lab/reasoning_embedder/venv/lib/python3.12/site-packages/sentence_transformers/models/Transformer.py:80\u001b[39m, in \u001b[36mTransformer.__init__\u001b[39m\u001b[34m(self, model_name_or_path, max_seq_length, model_args, tokenizer_args, config_args, cache_dir, do_lower_case, tokenizer_name_or_path, backend)\u001b[39m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config_args \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     78\u001b[39m     config_args = {}\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m config, is_peft_model = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     81\u001b[39m \u001b[38;5;28mself\u001b[39m._load_model(model_name_or_path, config, cache_dir, backend, is_peft_model, **model_args)\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m max_seq_length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mmodel_max_length\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m tokenizer_args:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/development/lab/reasoning_embedder/venv/lib/python3.12/site-packages/sentence_transformers/models/Transformer.py:145\u001b[39m, in \u001b[36mTransformer._load_config\u001b[39m\u001b[34m(self, model_name_or_path, cache_dir, backend, config_args)\u001b[39m\n\u001b[32m    141\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpeft\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PeftConfig\n\u001b[32m    143\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m PeftConfig.from_pretrained(model_name_or_path, **config_args, cache_dir=cache_dir), \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m145\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mAutoConfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mconfig_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/development/lab/reasoning_embedder/venv/lib/python3.12/site-packages/transformers/models/auto/configuration_auto.py:1073\u001b[39m, in \u001b[36mAutoConfig.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[39m\n\u001b[32m   1071\u001b[39m         config_class = CONFIG_MAPPING[config_dict[\u001b[33m\"\u001b[39m\u001b[33mmodel_type\u001b[39m\u001b[33m\"\u001b[39m]]\n\u001b[32m   1072\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1073\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1074\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe checkpoint you are trying to load has model type `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig_dict[\u001b[33m'\u001b[39m\u001b[33mmodel_type\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m` \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1075\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mbut Transformers does not recognize this architecture. This could be because of an \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1076\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33missue with the checkpoint, or because your version of Transformers is out of date.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1077\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mYou can update Transformers with the command `pip install --upgrade transformers`. If this \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1078\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mdoes not work, and the checkpoint is very new, then there may not be a release version \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1079\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mthat supports this model yet. In this case, you can get the most up-to-date code by installing \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1080\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mTransformers from source with the command \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1081\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m`pip install git+https://github.com/huggingface/transformers.git`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1082\u001b[39m         )\n\u001b[32m   1083\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m config_class.from_dict(config_dict, **unused_kwargs)\n\u001b[32m   1084\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1085\u001b[39m     \u001b[38;5;66;03m# Fallback: use pattern matching on the string.\u001b[39;00m\n\u001b[32m   1086\u001b[39m     \u001b[38;5;66;03m# We go from longer names to shorter names to catch roberta before bert (for instance)\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: The checkpoint you are trying to load has model type `qwen3` but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date.\n\nYou can update Transformers with the command `pip install --upgrade transformers`. If this does not work, and the checkpoint is very new, then there may not be a release version that supports this model yet. In this case, you can get the most up-to-date code by installing Transformers from source with the command `pip install git+https://github.com/huggingface/transformers.git`"
     ]
    }
   ],
   "source": [
    "# Load embedding model (minimal)\n",
    "if _USE_ST:\n",
    "    model = SentenceTransformer(MODEL_NAME)\n",
    "    def encode_texts(texts: List[str]) -> np.ndarray:\n",
    "        embs = model.encode(texts, convert_to_numpy=True, show_progress_bar=False, normalize_embeddings=True)\n",
    "        return embs.astype(np.float32)\n",
    "else:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    base_model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "    if DEVICE != 'cpu':\n",
    "        base_model = base_model.to(DEVICE)\n",
    "    base_model.eval()\n",
    "    import torch\n",
    "    @torch.no_grad()\n",
    "    def encode_texts(texts: List[str]) -> np.ndarray:\n",
    "        all_embs = []\n",
    "        for i in range(0, len(texts), 32):\n",
    "            batch = texts[i:i+32]\n",
    "            inputs = tokenizer(batch, padding=True, truncation=True, return_tensors='pt')\n",
    "            if DEVICE != 'cpu':\n",
    "                inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n",
    "            outputs = base_model(**inputs)\n",
    "            # Use last hidden state CLS pooling (minimal); some embedding models expose specific methods\n",
    "            hidden = outputs.last_hidden_state[:, 0, :]\n",
    "            # L2 normalize\n",
    "            hidden = torch.nn.functional.normalize(hidden, p=2, dim=1)\n",
    "            all_embs.append(hidden.detach().cpu().float())\n",
    "        embs = torch.cat(all_embs, dim=0).numpy()\n",
    "        return embs.astype(np.float32)\n",
    "\n",
    "print('Model ready.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14648ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a tiny cosine-sim index\n",
    "if corpus_texts:\n",
    "    doc_embs = encode_texts(corpus_texts)  # shape [N, D], normalized\n",
    "    # Ensure normalized (some encoders already normalize)\n",
    "    norms = np.linalg.norm(doc_embs, axis=1, keepdims=True) + 1e-8\n",
    "    doc_embs = doc_embs / norms\n",
    "else:\n",
    "    doc_embs = np.zeros((0, 1), dtype=np.float32)\n",
    "\n",
    "def search(query: str, top_k: int = TOP_K) -> List[Tuple[int, float]]:\n",
    "    if len(corpus_texts) == 0:\n",
    "        return []\n",
    "    q = encode_texts([query])[0]\n",
    "    q = q / (np.linalg.norm(q) + 1e-8)\n",
    "    sims = (doc_embs @ q)  # cosine since both normalized\n",
    "    idx = np.argpartition(-sims, min(top_k, len(sims)-1))[:top_k]\n",
    "    idx = idx[np.argsort(-sims[idx])]\n",
    "    return [(int(i), float(sims[i])) for i in idx]\n",
    "\n",
    "print(f'Index built with {len(corpus_texts)} docs.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757c3d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the index (edit `query` and re-run this cell)\n",
    "query = 'how to structure reasoning steps for QA over documents?'\n",
    "hits = search(query, top_k=TOP_K)\n",
    "print(f'Query: {query}\n",
    "')\n",
    "for rank, (i, score) in enumerate(hits, 1):\n",
    "    text = corpus_texts[i]\n",
    "    preview = (text[:400] + '...') if len(text) > 400 else text\n",
    "    print(f'#{rank} | score={score:.4f} | doc_id={i}')\n",
    "    print(preview)\n",
    "    print('-'*80)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (reasoning_embedder)",
   "language": "python",
   "name": "reasoning_embedder"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
