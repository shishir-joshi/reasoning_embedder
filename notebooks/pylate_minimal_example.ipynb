{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding the `pylate` API: A Minimal Example\n",
    "\n",
    "This notebook provides a minimal, step-by-step guide to using the `pylate` library. We will cover the essential components of the API, including:\n",
    "\n",
    "1.  **Setup and Imports**: Getting your environment ready.\n",
    "2.  **Loading a Model**: How to load a pre-trained ColBERT-style model.\n",
    "3.  **Inference (Encoding & Retrieval)**: How to encode documents and queries, build an index, and retrieve relevant documents.\n",
    "4.  **Fine-Tuning**: How to fine-tune a base model on a custom dataset using the `SentenceTransformerTrainer`.\n",
    "5.  **Saving and Loading**: How to save your fine-tuned model and load it back for later use.\n",
    "\n",
    "By the end of this notebook, you will have a clear understanding of the core workflow for both using and training models with `pylate`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setup and Imports\n",
    "\n",
    "First, let's import the necessary libraries. We need `torch` for tensor operations, `datasets` to handle our data, and various modules from `pylate` and `sentence_transformers` for modeling, training, and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⚠️ **Python 3.12 Compatibility Note**\n",
    "\n",
    "`torch.compile` (Torch Dynamo) is **not supported on Python 3.12+**.  \n",
    "ModernBERT (used inside the ColBERT family) decorates some internal\n",
    "functions with `@torch.compile`, which raises a `RuntimeError` under\n",
    "Python 3.12.  \n",
    "\n",
    "For production or training workloads we **strongly recommend** creating a\n",
    "virtual environment with Python 3.10 (or 3.9 / 3.11):\n",
    "```bash\n",
    "conda create -n pylate-310 python=3.10 pytorch torchvision -c pytorch\n",
    "conda activate pylate-310\n",
    "pip install pylate sentence-transformers datasets plaid-index\n",
    "```\n",
    "The next code-cell patches `torch.compile` so this notebook can still run on\n",
    "Python 3.12, **but JIT acceleration will be disabled**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Running on Python 3.10.18. torch.compile intact.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# Disable torch.compile on Python 3.12+ to avoid Dynamo errors\n",
    "# -------------------------------------------------------------\n",
    "if sys.version_info >= (3, 12):\n",
    "    if hasattr(torch, \"compile\"):\n",
    "        def _compile_noop(model=None, *args, **kwargs):\n",
    "            \"\"\"Fallback replacement that returns the original model/function.\"\"\"\n",
    "            return model if model is not None else (lambda x: x)\n",
    "        torch.compile = _compile_noop\n",
    "        print(\"[INFO] torch.compile disabled (Python 3.12+ detected). \"\n",
    "              \"Consider using Python 3.10 for full Torch Dynamo support.\")\n",
    "else:\n",
    "    print(f\"[INFO] Running on Python {sys.version.split()[0]}. torch.compile intact.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/reasoning_py310/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import Dataset\n",
    "from sentence_transformers import (\n",
    "    SentenceTransformerTrainer,\n",
    "    SentenceTransformerTrainingArguments,\n",
    ")\n",
    "\n",
    "from pylate import evaluation, losses, models, utils, retrieve, indexes\n",
    "import os\n",
    "\n",
    "# Create an output directory for our fine-tuned model\n",
    "os.makedirs(\"output/pylate-minimal-example\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Loading a Pre-trained Model\n",
    "\n",
    "`pylate` makes it easy to load any ColBERT-style model from the Hugging Face Hub. We'll use `lightonai/GTE-ModernColBERT-v1`, which is the base model for the `Reason-ModernColBERT` we aim to recreate.\n",
    "\n",
    "The `pylate.models.ColBERT` class handles the model architecture. It wraps a standard transformer model and adds the necessary layers for late-interaction retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-trained model...\n",
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading pre-trained model...\")\n",
    "model_id = \"lightonai/GTE-ModernColBERT-v1\"\n",
    "model = models.ColBERT(model_name_or_path=model_id, device=\"mps\")\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Inference: Encoding, Indexing, and Retrieval\n",
    "\n",
    "Now that we have a model, let's use it for its primary purpose: retrieval. This is a three-step process:\n",
    "1.  **Encode Documents**: Convert your document collection into vector representations.\n",
    "2.  **Index Documents**: Store these vectors in an efficient search index.\n",
    "3.  **Encode Query & Retrieve**: Convert a search query into a vector and use it to find the most similar documents in the index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1. Document Preparation & Encoding\n",
    "\n",
    "ColBERT is a \"late-interaction\" model, which means it represents documents as a set of vectors (one for each token) rather than a single vector. This preserves more granular information.\n",
    "\n",
    "When encoding, we must tell the model whether we are encoding a **query** or a **document**. This is done with the `is_query` flag.\n",
    "\n",
    "**`is_query=False`**: Use this for documents. The model will process the text and may apply document-specific padding or truncation.\n",
    "**`is_query=True`**: Use this for queries. The model will process the text and may apply query-specific tokens or processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding documents (bs=32): 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded 4 documents.\n",
      "Shape of the first document's embedding: (22, 128)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "documents = [\n",
    "    \"The Eiffel Tower is a wrought-iron lattice tower on the Champ de Mars in Paris, France.\",\n",
    "    \"Photosynthesis is a process used by plants, algae, and certain bacteria to convert light energy into chemical energy.\",\n",
    "    \"A CPU, or Central Processing Unit, is the primary component of a computer that executes instructions.\",\n",
    "    \"Paris is the capital and most populous city of France.\"\n",
    "]\n",
    "document_ids = [\"doc1\", \"doc2\", \"doc3\", \"doc4\"]\n",
    "\n",
    "print(\"Encoding documents...\")\n",
    "document_embeddings = model.encode(\n",
    "    documents,\n",
    "    is_query=False,  # Critical for encoding documents\n",
    "    show_progress_bar=True\n",
    ")\n",
    "\n",
    "print(f\"Encoded {len(document_embeddings)} documents.\")\n",
    "print(\"Shape of the first document's embedding:\", document_embeddings[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2. Indexing Documents\n",
    "\n",
    "To perform fast retrieval over thousands or millions of documents, we need to store their embeddings in a specialized index. `pylate` integrates with efficient index libraries. Here, we use `PLAID`, which is optimized for ColBERT's multi-vector representations.\n",
    "\n",
    "We'll create an in-memory index for this example. For larger collections, you can specify a folder to persist the index to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating an in-memory PLAID index...\n",
      "Adding documents to the index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding documents to the index (bs=2000): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 94.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents added successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating an in-memory PLAID index...\")\n",
    "index = indexes.Voyager(\n",
    "    index_folder=\"output/pylate-minimal-index\", # Directory to store index files\n",
    "    index_name=\"minimal_example_index\",\n",
    "    override=True,  # Overwrite if it already exists\n",
    "    # num_partitions=8,  # Use only 8 centroids – suitable for our tiny toy set\n",
    ")\n",
    "\n",
    "print(\"Adding documents to the index...\")\n",
    "index.add_documents(\n",
    "    documents_ids=document_ids,\n",
    "    documents_embeddings=document_embeddings,\n",
    ")\n",
    "print(\"Documents added successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3. Query Encoding & Retrieval\n",
    "\n",
    "Now we encode our search query using `is_query=True` and use the `ColBERT` retriever to search the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding query...\n",
      "Performing retrieval...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving documents (bs=50): 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Search Results:\n",
      "  Document ID: doc4, Score: 29.9538\n",
      "  Document ID: doc1, Score: 29.5036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the capital of France?\"\n",
    "\n",
    "print(\"Encoding query...\")\n",
    "query_embedding = model.encode(\n",
    "    [query], # Note: encode expects a list of strings\n",
    "    is_query=True # Critical for encoding queries\n",
    ")\n",
    "\n",
    "# Initialize the retriever with our index\n",
    "retriever = retrieve.ColBERT(index=index)\n",
    "\n",
    "print(\"Performing retrieval...\")\n",
    "search_results = retriever.retrieve(\n",
    "    queries_embeddings=query_embedding,\n",
    "    k=2 # Retrieve the top 2 most relevant documents\n",
    ")\n",
    "\n",
    "print(\"\\nSearch Results:\")\n",
    "for hit in search_results[0]: # Results for the first (and only) query\n",
    "    print(f\"  Document ID: {hit['id']}, Score: {hit['score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Fine-Tuning a Model\n",
    "\n",
    "While pre-trained models are powerful, fine-tuning them on a domain-specific dataset can significantly boost performance. The training process in `pylate` is built on the `sentence-transformers` `Trainer` API, making it familiar and robust.\n",
    "\n",
    "The key components are:\n",
    "- **Dataset**: A collection of training examples, typically triplets of (query, positive_document, negative_document).\n",
    "- **Model**: The base model to be fine-tuned.\n",
    "- **Loss Function**: A function that calculates how \"wrong\" the model's predictions are, guiding it to improve. For retrieval, `Contrastive` loss is common.\n",
    "- **Trainer**: An object that orchestrates the entire training loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1. Preparing a Training Dataset\n",
    "\n",
    "We'll create a tiny dataset of triplets. Each triplet consists of:\n",
    "- `query`: The search query.\n",
    "- `positive`: A document that is relevant to the query.\n",
    "- `negative`: A document that is *not* relevant to the query.\n",
    "\n",
    "The model learns to score the `(query, positive)` pair higher than the `(query, negative)` pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample training data:\n",
      "{'query': 'What is the capital of France?', 'positive': 'Paris is the capital and most populous city of France.', 'negative': 'The Eiffel Tower is a wrought-iron lattice tower on the Champ de Mars in Paris, France.'}\n"
     ]
    }
   ],
   "source": [
    "train_samples = {\n",
    "    \"query\": [\n",
    "        \"What is the capital of France?\", \n",
    "        \"What does a CPU do?\"\n",
    "    ],\n",
    "    \"positive\": [\n",
    "        \"Paris is the capital and most populous city of France.\",\n",
    "        \"A CPU, or Central Processing Unit, is the primary component of a computer that executes instructions.\"\n",
    "    ],\n",
    "    \"negative\": [\n",
    "        \"The Eiffel Tower is a wrought-iron lattice tower on the Champ de Mars in Paris, France.\", # Related but not the direct answer\n",
    "        \"Photosynthesis is a process used by plants to convert light energy into chemical energy.\" # Unrelated\n",
    "    ]\n",
    "}\n",
    "\n",
    "train_dataset = Dataset.from_dict(train_samples)\n",
    "eval_dataset = Dataset.from_dict(train_samples) # Using the same for simplicity\n",
    "\n",
    "print(\"Sample training data:\")\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2. Setting up Training Components\n",
    "\n",
    "Now we define the model, loss, evaluator, and training arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The 'main_distance_function' parameter is deprecated. Please use 'main_similarity_function' instead. 'main_distance_function' will be removed in a future release.\n",
      "/opt/anaconda3/envs/reasoning_py310/lib/python3.10/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 1. Model: We'll fine-tune the same model we loaded earlier.\n",
    "# For a real scenario, you might start from a more general base like 'bert-base-uncased'.\n",
    "training_model = models.ColBERT(model_name_or_path=model_id)\n",
    "\n",
    "# 2. Loss Function: Contrastive loss pushes positive pairs closer and negative pairs further apart.\n",
    "# A temperature around 0.02 is often a good starting point.\n",
    "train_loss = losses.Contrastive(model=training_model, temperature=0.02)\n",
    "\n",
    "# 3. Evaluator: This will compute metrics on the evaluation set during training.\n",
    "dev_evaluator = evaluation.ColBERTTripletEvaluator(\n",
    "    anchors=eval_dataset[\"query\"],\n",
    "    positives=eval_dataset[\"positive\"],\n",
    "    negatives=eval_dataset[\"negative\"],\n",
    ")\n",
    "\n",
    "# 4. Data Collator: This prepares batches of data for the ColBERT model.\n",
    "data_collator = utils.ColBERTCollator(training_model.tokenize)\n",
    "\n",
    "# 5. Training Arguments: Configure the training process.\n",
    "output_dir = \"output/pylate-minimal-example\"\n",
    "args = SentenceTransformerTrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    fp16=torch.cuda.is_available(),  # Use mixed precision if a GPU is available\n",
    "    save_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_steps=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. The Training Loop\n",
    "\n",
    "With all the components ready, we can initialize the `SentenceTransformerTrainer` and start the fine-tuning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fine-tuning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/reasoning_py310/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 00:12, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/reasoning_py310/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning complete!\n"
     ]
    }
   ],
   "source": [
    "trainer = SentenceTransformerTrainer(\n",
    "    model=training_model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    loss=train_loss,\n",
    "    evaluator=dev_evaluator,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"Starting fine-tuning...\")\n",
    "# This will run the training loop. It will take a few moments even on this tiny dataset.\n",
    "trainer.train()\n",
    "print(\"Fine-tuning complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Saving and Loading the Fine-Tuned Model\n",
    "\n",
    "The `Trainer` automatically saves the final model checkpoints in the specified `output_dir`. You can easily load this model for inference, just like you loaded the pre-trained model from the Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved in: output/pylate-minimal-example\n",
      "\n",
      "Loading fine-tuned model...\n",
      "Fine-tuned model loaded successfully.\n",
      "\n",
      "Successfully encoded a query with the fine-tuned model.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model saved in: {output_dir}\")\n",
    "\n",
    "# Load the fine-tuned model from the output directory\n",
    "print(\"\\nLoading fine-tuned model...\")\n",
    "# SentenceTransformerTrainer saves each epoch to a numbered checkpoint\n",
    "# (e.g. output_dir/checkpoint-1).  We load from that sub-directory.\n",
    "checkpoint_dir = f\"{output_dir}/checkpoint-1\"\n",
    "fine_tuned_model = models.ColBERT(model_name_or_path=checkpoint_dir)\n",
    "print(\"Fine-tuned model loaded successfully.\")\n",
    "\n",
    "# You can now use this model for inference just like before\n",
    "fine_tuned_query_embedding = fine_tuned_model.encode(\n",
    "    [query],\n",
    "    is_query=True\n",
    ")\n",
    "print(\"\\nSuccessfully encoded a query with the fine-tuned model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "This notebook has walked you through the fundamental API of the `pylate` library. You've learned how to:\n",
    "- Load a pre-trained ColBERT model.\n",
    "- Encode documents and queries correctly.\n",
    "- Build a retrieval index and search it.\n",
    "- Set up a complete fine-tuning pipeline.\n",
    "- Train a model and save the result.\n",
    "\n",
    "With this foundation, you are now ready to apply `pylate` to larger, real-world datasets and build powerful, reasoning-intensive retrieval systems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (reasoning)",
   "language": "python",
   "name": "reasoning_py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
