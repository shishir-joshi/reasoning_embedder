{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BRIGHT and NanoBEIR Benchmark Evaluation\n",
    "\n",
    "This notebook implements the evaluation of embedding models on the BRIGHT benchmark, following the original author's reference implementation. We evaluate both late-interaction models (ColBERT-based) and dense models.\n",
    "\n",
    "## Reference Implementation\n",
    "This notebook follows the methodology from the official evaluation script: https://gist.github.com/NohTow/3f27d2816b92d5c76f0e63aa7757cf4b\n",
    "\n",
    "## Target Models\n",
    "- **Late-Interaction Models**: `lightonai/Reason-ModernColBERT`, `lightonai/GTE-ModernColBERT-v1`\n",
    "- **Dense Models**: `jinaai/jina-embeddings-v3`, `Qwen/Qwen3-Embedding-0.6B`\n",
    "\n",
    "## Benchmarks\n",
    "- **BRIGHT**: A comprehensive benchmark for reasoning-intensive retrieval tasks\n",
    "- **NanoBEIR**: A collection of standard retrieval tasks for comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Required Dependencies\n",
    "\n",
    "First, we need to install the required libraries. This includes `mteb` for loading the BRIGHT tasks and `pylate` for the ColBERT evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install mteb pylate srsly psutil torch sentence-transformers matplotlib seaborn pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('attempt to write a readonly database')).History will not be written to the database.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/reasoning_py310/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import datetime\n",
    "import traceback\n",
    "import gc\n",
    "import psutil\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from typing import Dict, List, Any, Tuple, Union, Optional\n",
    "\n",
    "import mteb\n",
    "import srsly\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "# Import PyLate modules for ColBERT evaluation\n",
    "from pylate import evaluation, indexes, models, retrieve\n",
    "\n",
    "# Set up configuration\n",
    "DEBUG_MODE = True  # Set to False for full evaluation\n",
    "OUTPUT_DIR = \"bright_evaluation_results\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Memory and Logging Utilities\n",
    "\n",
    "These utilities help track memory usage and provide detailed logging throughout the evaluation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_memory_usage():\n",
    "    \"\"\"Get current memory usage of the process\"\"\"\n",
    "    process = psutil.Process(os.getpid())\n",
    "    return {\n",
    "        \"ram_gb\": process.memory_info().rss / (1024 ** 3),\n",
    "        \"ram_percent\": psutil.virtual_memory().percent\n",
    "    }\n",
    "\n",
    "def get_gpu_memory_usage():\n",
    "    \"\"\"Get current GPU memory usage if available\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return {\n",
    "            \"allocated_gb\": torch.cuda.memory_allocated() / (1024 ** 3),\n",
    "            \"reserved_gb\": torch.cuda.memory_reserved() / (1024 ** 3),\n",
    "            \"max_allocated_gb\": torch.cuda.max_memory_allocated() / (1024 ** 3)\n",
    "        }\n",
    "    return {\"gpu_available\": False}\n",
    "\n",
    "def log_with_timestamp(message, level=\"INFO\"):\n",
    "    \"\"\"Log a message with timestamp and log level\"\"\"\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    prefix = f\"[{timestamp}] [{level}]\"\n",
    "    print(f\"{prefix} {message}\")\n",
    "\n",
    "def log_memory_status(tag=\"\"):\n",
    "    \"\"\"Log current memory usage with optional tag\"\"\"\n",
    "    mem = get_memory_usage()\n",
    "    log_with_timestamp(f\"Memory Status {tag} - RAM: {mem['ram_gb']:.2f} GB ({mem['ram_percent']}%)\", \"MEMORY\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        gpu_mem = get_gpu_memory_usage()\n",
    "        log_with_timestamp(f\"GPU Memory {tag} - Allocated: {gpu_mem['allocated_gb']:.2f} GB, \"\n",
    "                         f\"Reserved: {gpu_mem['reserved_gb']:.2f} GB, \"\n",
    "                         f\"Max: {gpu_mem['max_allocated_gb']:.2f} GB\", \"MEMORY\")\n",
    "\n",
    "def cleanup_memory():\n",
    "    \"\"\"Clean up memory and GPU cache\"\"\"\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    if torch.mps.is_available():\n",
    "        torch.mps.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Configuration\n",
    "\n",
    "Define the models to evaluate and their configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-01 18:36:08] [CONFIG] Not running in Google Colab environment\n"
     ]
    }
   ],
   "source": [
    "# Models to evaluate - temporarily remove problematic ones\n",
    "MODELS_TO_TEST = [\n",
    "    {\n",
    "        \"name\": \"Reason-ModernColBERT\",\n",
    "        \"model_id\": \"lightonai/Reason-ModernColBERT\",\n",
    "        \"query_length\": 128,\n",
    "        \"type\": \"ColBERT\",\n",
    "        \"device\": \"mps\"  # Force CPU to avoid OOM issues\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"GTE-ModernColBERT-v1\", \n",
    "        \"model_id\": \"lightonai/GTE-ModernColBERT-v1\",\n",
    "        \"query_length\": 128,\n",
    "        \"type\": \"ColBERT\",\n",
    "        \"device\": \"mps\"  # Force CPU to avoid OOM issues\n",
    "    }\n",
    "    # Temporarily comment out problematic models\n",
    "    # {\n",
    "    #     \"name\": \"Jina-Embeddings-v3\",\n",
    "    #     \"model_id\": \"jinaai/jina-embeddings-v3\",\n",
    "    #     \"type\": \"Dense\",\n",
    "    #     \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    # },\n",
    "    # {\n",
    "    #     \"name\": \"Qwen3-Embedding-0.6B\",\n",
    "    #     \"model_id\": \"Qwen/Qwen3-Embedding-0.6B\",\n",
    "    #     \"type\": \"Dense\",\n",
    "    #     \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    #     \"trust_remote_code\": True\n",
    "    # }\n",
    "]\n",
    "\n",
    "# Check if we're in Google Colab and adjust device settings\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    log_with_timestamp(\"Running in Google Colab environment\", \"CONFIG\")\n",
    "    \n",
    "    # Check available GPU\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_info = !nvidia-smi\n",
    "        log_with_timestamp(f\"GPU available: {torch.cuda.get_device_name(0)}\", \"CONFIG\")\n",
    "        \n",
    "        # Update device settings for models that can run on GPU\n",
    "        for model in MODELS_TO_TEST:\n",
    "            if model[\"type\"] == \"Dense\":\n",
    "                model[\"device\"] = \"cuda\"\n",
    "    else:\n",
    "        log_with_timestamp(\"No GPU available in Colab\", \"WARNING\")\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    log_with_timestamp(\"Not running in Google Colab environment\", \"CONFIG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. BRIGHT Evaluation Functions\n",
    "\n",
    "These functions implement the BRIGHT evaluation following the author's reference implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_colbert_on_bright(model_config: Dict[str, Any], eval_sets: List[str] = None) -> Dict[str, Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Evaluate a ColBERT model on BRIGHT benchmark following the author's methodology\n",
    "    \n",
    "    Args:\n",
    "        model_config: Dictionary with model configuration\n",
    "        eval_sets: List of evaluation sets to run (if None, runs all available)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with results for each eval_set\n",
    "    \"\"\"\n",
    "    model_name = model_config[\"model_id\"]\n",
    "    query_length = model_config.get(\"query_length\", 128)\n",
    "    model_short_name = model_config[\"name\"]\n",
    "    device = model_config.get(\"device\", \"cpu\")\n",
    "    \n",
    "    log_with_timestamp(f\"Starting evaluation for {model_name}\", \"START\")\n",
    "    log_memory_status(\"initial\")\n",
    "    \n",
    "    # Load BRIGHT tasks using MTEB\n",
    "    log_with_timestamp(\"Loading BRIGHT tasks from MTEB\", \"DATA\")\n",
    "    tasks = mteb.get_tasks(tasks=[\"BrightRetrieval\"])\n",
    "    task = tasks[0]\n",
    "    task.load_data()\n",
    "    \n",
    "    log_with_timestamp(f\"Loaded BRIGHT task with eval sets: {list(task.queries.keys())}\", \"DATA\")\n",
    "    \n",
    "    # Determine which eval sets to run\n",
    "    if eval_sets is None:\n",
    "        available_sets = list(task.queries.keys())\n",
    "        if DEBUG_MODE:\n",
    "            # In debug mode, just run first 2 sets\n",
    "            eval_sets_to_run = available_sets[:2]\n",
    "            log_with_timestamp(f\"Debug mode: Running on {eval_sets_to_run}\", \"DEBUG\")\n",
    "        else:\n",
    "            eval_sets_to_run = available_sets\n",
    "    else:\n",
    "        eval_sets_to_run = eval_sets\n",
    "    \n",
    "    # Initialize model\n",
    "    log_with_timestamp(f\"Initializing model: {model_name} with query_length={query_length}\", \"MODEL\")\n",
    "    model = models.ColBERT(\n",
    "        model_name_or_path=model_name,\n",
    "        query_length=query_length,\n",
    "        device=device\n",
    "    )\n",
    "    log_with_timestamp(\"Model loaded successfully\", \"MODEL\")\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for eval_set in eval_sets_to_run:\n",
    "        eval_start_time = time.time()\n",
    "        log_with_timestamp(f\"Starting evaluation on set: {eval_set}\", \"EVAL\")\n",
    "        log_memory_status(f\"before {eval_set}\")\n",
    "        \n",
    "        try:\n",
    "            # Create output directory\n",
    "            output_dir = os.path.join(OUTPUT_DIR, f\"{model_short_name}_ir\")\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            \n",
    "            # Check if results already exist\n",
    "            result_file = os.path.join(\n",
    "                output_dir,\n",
    "                f\"{task.metadata.name}_{eval_set.replace('/', '_')}_evaluation_scores_qlen{query_length}.json\"\n",
    "            )\n",
    "            \n",
    "            if os.path.exists(result_file):\n",
    "                log_with_timestamp(f\"Results already exist for {eval_set}. Loading existing results.\", \"SKIP\")\n",
    "                with open(result_file, 'r') as f:\n",
    "                    evaluation_scores = json.load(f)\n",
    "                results[eval_set] = evaluation_scores\n",
    "                continue\n",
    "            \n",
    "            # Create index\n",
    "            log_with_timestamp(f\"Creating PLAID index for {eval_set}\", \"INDEX\")\n",
    "            index = indexes.PLAID(\n",
    "                override=True,\n",
    "                nbits=4,\n",
    "                index_name=f\"{task.metadata.name}_{eval_set}_{model_short_name}_{query_length}_4bits_ir\",\n",
    "            )\n",
    "            \n",
    "            # Get documents and queries\n",
    "            corpus = task.corpus[eval_set][\"standard\"] \n",
    "            queries = task.queries[eval_set][\"standard\"]\n",
    "            qrels = task.relevant_docs[eval_set][\"standard\"]\n",
    "            \n",
    "            # Handle excluded docs - check if it exists\n",
    "            excluded_docs = {}\n",
    "            if \"excluded\" in task.relevant_docs[eval_set]:\n",
    "                excluded_docs = task.relevant_docs[eval_set][\"excluded\"]\n",
    "                log_with_timestamp(f\"Found excluded docs for {eval_set}\", \"DATA\")\n",
    "            else:\n",
    "                # Create N/A entries for all queries if no excluded docs\n",
    "                excluded_docs = {qid: \"N/A\" for qid in queries.keys()}\n",
    "                log_with_timestamp(f\"No excluded docs found for {eval_set}, using N/A\", \"DATA\")\n",
    "            \n",
    "            log_with_timestamp(f\"Dataset stats - Docs: {len(corpus)}, Queries: {len(queries)}, Qrels: {len(qrels)}\", \"DATA\")\n",
    "            \n",
    "            # Sample for debug mode\n",
    "            if DEBUG_MODE:\n",
    "                max_docs = 1000\n",
    "                max_queries = 50\n",
    "                \n",
    "                # Sample documents\n",
    "                doc_ids = list(corpus.keys())\n",
    "                if len(doc_ids) > max_docs:\n",
    "                    sampled_doc_ids = doc_ids[:max_docs]\n",
    "                    corpus = {doc_id: corpus[doc_id] for doc_id in sampled_doc_ids}\n",
    "                    log_with_timestamp(f\"Sampled corpus to {len(corpus)} documents\", \"DEBUG\")\n",
    "                \n",
    "                # Sample queries\n",
    "                query_ids = list(queries.keys())\n",
    "                if len(query_ids) > max_queries:\n",
    "                    sampled_query_ids = query_ids[:max_queries]\n",
    "                    queries = {qid: queries[qid] for qid in sampled_query_ids}\n",
    "                    qrels = {qid: qrels[qid] for qid in sampled_query_ids if qid in qrels}\n",
    "                    excluded_docs = {qid: excluded_docs.get(qid, \"N/A\") for qid in sampled_query_ids}\n",
    "                    log_with_timestamp(f\"Sampled to {len(queries)} queries\", \"DEBUG\")\n",
    "            \n",
    "            # Encode documents\n",
    "            log_with_timestamp(\"Encoding documents...\", \"ENCODE\")\n",
    "            documents_embeddings = model.encode(\n",
    "                sentences=list(corpus.values()),\n",
    "                batch_size=50,  # Smaller batch size to avoid OOM\n",
    "                is_query=False,\n",
    "                show_progress_bar=True,\n",
    "            )\n",
    "            log_with_timestamp(\"Document encoding completed\", \"ENCODE\")\n",
    "            \n",
    "            # Add documents to index\n",
    "            log_with_timestamp(\"Adding documents to index...\", \"INDEX\")\n",
    "            index.add_documents(\n",
    "                documents_ids=list(corpus.keys()),\n",
    "                documents_embeddings=documents_embeddings,\n",
    "            )\n",
    "            log_with_timestamp(\"Documents added to index\", \"INDEX\")\n",
    "            \n",
    "            # Create retriever\n",
    "            retriever = retrieve.ColBERT(index=index)\n",
    "            \n",
    "            # Encode queries\n",
    "            log_with_timestamp(\"Encoding queries...\", \"ENCODE\")\n",
    "            queries_embeddings = model.encode(\n",
    "                sentences=list(queries.values()),\n",
    "                is_query=True,\n",
    "                show_progress_bar=True,\n",
    "                batch_size=16,  # Smaller batch size for queries\n",
    "            )\n",
    "            log_with_timestamp(\"Query encoding completed\", \"ENCODE\")\n",
    "            \n",
    "            # Retrieve\n",
    "            log_with_timestamp(\"Retrieving results...\", \"RETRIEVE\")\n",
    "            scores = retriever.retrieve(queries_embeddings=queries_embeddings, k=100)\n",
    "            log_with_timestamp(\"Retrieval completed\", \"RETRIEVE\")\n",
    "            \n",
    "            # Filter excluded documents\n",
    "            log_with_timestamp(\"Filtering excluded documents...\", \"FILTER\")\n",
    "            filtered_scores = []\n",
    "            \n",
    "            for query_scores, excluded_ids in zip(scores, excluded_docs.values()):\n",
    "                # Some splits have no excluded ids\n",
    "                if excluded_ids == \"N/A\" or not excluded_ids:\n",
    "                    filtered_scores.append(query_scores)\n",
    "                    continue\n",
    "                \n",
    "                filtered_query_scores = []\n",
    "                for query_score in query_scores:\n",
    "                    if query_score[\"id\"] in excluded_ids:\n",
    "                        continue\n",
    "                    filtered_query_scores.append(query_score)\n",
    "                filtered_scores.append(filtered_query_scores)\n",
    "            \n",
    "            log_with_timestamp(\"Exclusion filtering completed\", \"FILTER\")\n",
    "            \n",
    "            # Evaluate\n",
    "            log_with_timestamp(\"Computing evaluation metrics...\", \"METRICS\") \n",
    "            evaluation_scores = evaluation.evaluate(\n",
    "                scores=filtered_scores,\n",
    "                qrels=qrels,\n",
    "                queries=list(queries.keys()),\n",
    "                metrics=[\"map\", \"ndcg@1\", \"ndcg@10\", \"ndcg@100\", \"recall@10\", \"recall@100\"],\n",
    "            )\n",
    "            log_with_timestamp(\"Evaluation completed\", \"METRICS\")\n",
    "            \n",
    "            # Save results\n",
    "            srsly.write_json(result_file, evaluation_scores)\n",
    "            log_with_timestamp(f\"Results saved to {result_file}\", \"SAVE\")\n",
    "            \n",
    "            # Store results\n",
    "            results[eval_set] = evaluation_scores\n",
    "            \n",
    "            # Log key scores\n",
    "            ndcg_10 = evaluation_scores.get(\"ndcg@10\", {}).get(\"mean\", 0.0) * 100\n",
    "            eval_time = time.time() - eval_start_time\n",
    "            log_with_timestamp(f\"Completed {eval_set}: nDCG@10 = {ndcg_10:.4f} (took {eval_time:.2f}s)\", \"SUCCESS\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error evaluating {model_name} on {eval_set}: {str(e)}\"\n",
    "            log_with_timestamp(error_msg, \"ERROR\")\n",
    "            log_with_timestamp(f\"Traceback:\\n{traceback.format_exc()}\", \"ERROR\")\n",
    "            \n",
    "            # Store error result\n",
    "            results[eval_set] = {\n",
    "                \"error\": str(e),\n",
    "                \"ndcg@10\": {\"mean\": 0.0}\n",
    "            }\n",
    "        \n",
    "        # Cleanup memory after each eval set\n",
    "        try:\n",
    "            cleanup_memory()\n",
    "            log_with_timestamp(\"Memory cleanup completed\", \"CLEANUP\")\n",
    "            log_memory_status(f\"after {eval_set}\")\n",
    "        except Exception as cleanup_error:\n",
    "            log_with_timestamp(f\"Cleanup warning: {str(cleanup_error)}\", \"WARNING\")\n",
    "    \n",
    "    # Cleanup model\n",
    "    del model\n",
    "    cleanup_memory()\n",
    "    \n",
    "    log_with_timestamp(f\"Completed evaluation for {model_name}\", \"COMPLETE\")\n",
    "    return results\n",
    "\n",
    "def evaluate_dense_model_mteb(model_config: Dict[str, Any]) -> Dict[str, Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Evaluate a dense model using MTEB's BRIGHT task directly\n",
    "    \n",
    "    Args:\n",
    "        model_config: Dictionary with model configuration\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with results for each eval_set\n",
    "    \"\"\"\n",
    "    model_id = model_config[\"model_id\"]\n",
    "    model_name = model_config[\"name\"]\n",
    "    device = model_config.get(\"device\", \"cpu\")\n",
    "    trust_remote_code = model_config.get(\"trust_remote_code\", False)\n",
    "    \n",
    "    log_with_timestamp(f\"Starting MTEB evaluation for dense model {model_id}\", \"START\")\n",
    "    log_memory_status(\"initial\")\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    try:\n",
    "        # Load model with special handling for Jina\n",
    "        log_with_timestamp(f\"Loading model {model_id} on {device}\", \"MODEL\")\n",
    "        \n",
    "        if \"jina\" in model_id.lower():\n",
    "            # Special handling for Jina models - use transformers directly\n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "            model = AutoModel.from_pretrained(model_id, trust_remote_code=True)\n",
    "            model.to(device)\n",
    "            \n",
    "            # Create a wrapper for sentence transformers compatibility\n",
    "            class JinaModelWrapper:\n",
    "                def __init__(self, model, tokenizer, device):\n",
    "                    self.model = model\n",
    "                    self.tokenizer = tokenizer\n",
    "                    self.device = device\n",
    "                    \n",
    "                def encode(self, sentences, batch_size=32, **kwargs):\n",
    "                    embeddings = []\n",
    "                    for i in range(0, len(sentences), batch_size):\n",
    "                        batch = sentences[i:i+batch_size]\n",
    "                        inputs = self.tokenizer(batch, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "                        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "                        \n",
    "                        with torch.no_grad():\n",
    "                            outputs = self.model(**inputs)\n",
    "                            # Use mean pooling\n",
    "                            embeddings_batch = outputs.last_hidden_state.mean(dim=1)\n",
    "                            embeddings_batch = F.normalize(embeddings_batch, p=2, dim=1)\n",
    "                            embeddings.append(embeddings_batch.cpu())\n",
    "                    \n",
    "                    return torch.cat(embeddings, dim=0).numpy()\n",
    "            \n",
    "            model_wrapper = JinaModelWrapper(model, tokenizer, device)\n",
    "            log_with_timestamp(\"Jina model loaded with custom wrapper\", \"MODEL\")\n",
    "            \n",
    "        else:\n",
    "            # Standard loading for other models\n",
    "            model_kwargs = {}\n",
    "            if trust_remote_code:\n",
    "                model_kwargs[\"trust_remote_code\"] = True\n",
    "                \n",
    "            model_wrapper = SentenceTransformer(model_id, device=device, **model_kwargs)\n",
    "            log_with_timestamp(\"Model loaded successfully\", \"MODEL\")\n",
    "        \n",
    "        # Get BRIGHT task - fix the import\n",
    "        log_with_timestamp(\"Loading BRIGHT task\", \"DATA\")\n",
    "        \n",
    "        # Use the correct way to get BRIGHT task\n",
    "        tasks = mteb.get_tasks(tasks=[\"BrightRetrieval\"])\n",
    "        task = tasks[0]\n",
    "        \n",
    "        # Determine which splits to evaluate\n",
    "        if DEBUG_MODE:\n",
    "            eval_splits = [\"test\"][:1]  # Just use first test split in debug mode\n",
    "            log_with_timestamp(f\"Debug mode: Using only {eval_splits} split\", \"DEBUG\")\n",
    "        else:\n",
    "            eval_splits = [\"test\"]\n",
    "            \n",
    "        # Run evaluation using MTEB framework\n",
    "        log_with_timestamp(f\"Running MTEB evaluation on {len(eval_splits)} splits\", \"EVAL\")\n",
    "        output_folder = os.path.join(OUTPUT_DIR, f\"mteb_{model_name}\")\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "        \n",
    "        # Create MTEB evaluation\n",
    "        evaluation_mteb = mteb.MTEB(tasks=[task])\n",
    "        mteb_results = evaluation_mteb.run(\n",
    "            model_wrapper, \n",
    "            output_folder=output_folder,\n",
    "            eval_splits=eval_splits,\n",
    "            overwrite_results=True\n",
    "        )\n",
    "        \n",
    "        # Process results\n",
    "        log_with_timestamp(\"Processing evaluation results\", \"RESULTS\")\n",
    "        \n",
    "        # Extract results from MTEB format\n",
    "        task_results = mteb_results[0]  # First (and only) task\n",
    "        \n",
    "        for eval_set, metrics in task_results.items():\n",
    "            if eval_set == \"test\":  # We're interested in test results\n",
    "                results[eval_set] = {}\n",
    "                for metric, value in metrics.items():\n",
    "                    if metric.lower() == \"ndcg_at_10\":\n",
    "                        results[eval_set][\"ndcg@10\"] = {\"mean\": value}\n",
    "                    else:\n",
    "                        results[eval_set][metric.lower()] = {\"mean\": value}\n",
    "                        \n",
    "                # Save results\n",
    "                result_file = os.path.join(output_folder, f\"{eval_set}_results.json\")\n",
    "                with open(result_file, 'w') as f:\n",
    "                    json.dump(results[eval_set], f, indent=2)\n",
    "                log_with_timestamp(f\"Results for {eval_set} saved to {result_file}\", \"SAVE\")\n",
    "                \n",
    "                # Log key scores\n",
    "                ndcg_10 = results[eval_set].get(\"ndcg@10\", {}).get(\"mean\", 0.0) * 100\n",
    "                log_with_timestamp(f\"Completed {eval_set}: nDCG@10 = {ndcg_10:.4f}\", \"SUCCESS\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error in MTEB evaluation for {model_id}: {str(e)}\"\n",
    "        log_with_timestamp(error_msg, \"ERROR\")\n",
    "        log_with_timestamp(f\"Traceback:\\n{traceback.format_exc()}\", \"ERROR\")\n",
    "        results[\"error\"] = {\"error\": str(e), \"ndcg@10\": {\"mean\": 0.0}}\n",
    "    \n",
    "    finally:\n",
    "        # Clean up\n",
    "        if 'model' in locals():\n",
    "            del model\n",
    "        if 'model_wrapper' in locals():\n",
    "            del model_wrapper\n",
    "        cleanup_memory()\n",
    "        log_with_timestamp(\"Memory cleanup completed\", \"CLEANUP\")\n",
    "        log_memory_status(\"final\")\n",
    "        \n",
    "    log_with_timestamp(f\"Completed MTEB evaluation for {model_id}\", \"COMPLETE\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Main Evaluation Pipeline\n",
    "\n",
    "This is the main function to run the complete BRIGHT benchmark evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_bright_benchmark():\n",
    "    \"\"\"Run the complete BRIGHT benchmark evaluation\"\"\"\n",
    "    log_with_timestamp(\"Starting BRIGHT benchmark evaluation\", \"START\")\n",
    "    \n",
    "    # Set up logging file\n",
    "    log_file = os.path.join(OUTPUT_DIR, \"bright_evaluation_log.txt\")\n",
    "    with open(log_file, \"w\") as f:\n",
    "        f.write(f\"BRIGHT Evaluation Log - Started at {datetime.datetime.now()}\\n\")\n",
    "        f.write(f\"Debug mode: {DEBUG_MODE}\\n\\n\")\n",
    "    \n",
    "    all_results = {}\n",
    "    results_summary = []\n",
    "    \n",
    "    for model_config in MODELS_TO_TEST:\n",
    "        log_with_timestamp(f\"Evaluating model: {model_config['name']}\", \"MODEL\")\n",
    "        \n",
    "        try:\n",
    "            # Evaluate based on model type\n",
    "            if model_config[\"type\"] == \"ColBERT\":\n",
    "                model_results = evaluate_colbert_on_bright(model_config)\n",
    "            else:  # Dense models\n",
    "                model_results = evaluate_dense_model_mteb(model_config)\n",
    "                \n",
    "            all_results[model_config[\"name\"]] = model_results\n",
    "            \n",
    "            # Extract scores for summary\n",
    "            for eval_set, scores in model_results.items():\n",
    "                if \"error\" not in scores:\n",
    "                    ndcg_10 = scores.get(\"ndcg@10\", {}).get(\"mean\", 0.0) * 100\n",
    "                else:\n",
    "                    ndcg_10 = 0.0\n",
    "                \n",
    "                results_summary.append({\n",
    "                    \"Model\": model_config[\"name\"],\n",
    "                    \"EvalSet\": eval_set,\n",
    "                    \"nDCG@10\": ndcg_10\n",
    "                })\n",
    "                \n",
    "        except Exception as e:\n",
    "            log_with_timestamp(f\"Failed to evaluate {model_config['name']}: {str(e)}\", \"ERROR\")\n",
    "            all_results[model_config[\"name\"]] = {\"error\": str(e)}\n",
    "            \n",
    "            # Add to log file\n",
    "            with open(log_file, \"a\") as f:\n",
    "                f.write(f\"\\nERROR: {model_config['name']} at {datetime.datetime.now()}\\n\")\n",
    "                f.write(f\"Error: {str(e)}\\n\")\n",
    "                f.write(traceback.format_exc() + \"\\n\\n\")\n",
    "    \n",
    "    # Create results DataFrame\n",
    "    results_df = pd.DataFrame(results_summary)\n",
    "    \n",
    "    # Save complete results\n",
    "    results_file = os.path.join(OUTPUT_DIR, \"bright_evaluation_complete_results.json\")\n",
    "    with open(results_file, 'w') as f:\n",
    "        json.dump(all_results, f, indent=2)\n",
    "    log_with_timestamp(f\"Complete results saved to {results_file}\", \"SAVE\")\n",
    "    \n",
    "    # Save summary\n",
    "    summary_file = os.path.join(OUTPUT_DIR, \"bright_evaluation_summary.csv\")\n",
    "    results_df.to_csv(summary_file, index=False)\n",
    "    log_with_timestamp(f\"Summary saved to {summary_file}\", \"SAVE\")\n",
    "    \n",
    "    return results_df, all_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Results Visualization\n",
    "\n",
    "Functions to visualize the benchmark results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_bright_results(results_df):\n",
    "    \"\"\"Create visualizations for BRIGHT results\"\"\"\n",
    "    if results_df.empty:\n",
    "        log_with_timestamp(\"No results to visualize\", \"WARNING\")\n",
    "        return\n",
    "    \n",
    "    log_with_timestamp(\"Creating visualizations\", \"PLOT\")\n",
    "    \n",
    "    # Per-task performance\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    sns.barplot(data=results_df, x=\"EvalSet\", y=\"nDCG@10\", hue=\"Model\")\n",
    "    plt.title(\"BRIGHT Benchmark Results by Evaluation Set\", fontsize=16)\n",
    "    plt.ylabel(\"nDCG@10 (%)\")\n",
    "    plt.xlabel(\"Evaluation Set\")\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.legend(title='Model')\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUTPUT_DIR, 'bright_results_by_task.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Average performance\n",
    "    model_avg = results_df.groupby('Model')['nDCG@10'].mean().reset_index()\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(data=model_avg, x=\"Model\", y=\"nDCG@10\")\n",
    "    plt.title(\"Average BRIGHT Performance Across All Tasks\", fontsize=16)\n",
    "    plt.ylabel(\"Average nDCG@10 (%)\")\n",
    "    plt.xlabel(\"Model\")\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUTPUT_DIR, 'bright_results_average.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Summary table\n",
    "    print(\"\\nBRIGHT Evaluation Summary:\")\n",
    "    summary_table = results_df.pivot_table(\n",
    "        values='nDCG@10', \n",
    "        index='Model', \n",
    "        columns='EvalSet', \n",
    "        aggfunc='mean'\n",
    "    )\n",
    "    summary_table['Average'] = summary_table.mean(axis=1)\n",
    "    display(summary_table.round(2))\n",
    "    \n",
    "    # Save summary table\n",
    "    summary_table.to_csv(os.path.join(OUTPUT_DIR, 'bright_summary_table.csv'))\n",
    "    \n",
    "    log_with_timestamp(\"Visualizations completed\", \"COMPLETE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Run the Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-01 18:36:26] [MAIN] Starting BRIGHT evaluation pipeline\n",
      "[2025-09-01 18:36:26] [START] Starting BRIGHT benchmark evaluation\n",
      "[2025-09-01 18:36:26] [MODEL] Evaluating model: Reason-ModernColBERT\n",
      "[2025-09-01 18:36:26] [START] Starting evaluation for lightonai/Reason-ModernColBERT\n",
      "[2025-09-01 18:36:26] [MEMORY] Memory Status initial - RAM: 0.13 GB (81.1%)\n",
      "[2025-09-01 18:36:26] [DATA] Loading BRIGHT tasks from MTEB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run the evaluation\n",
    "log_with_timestamp(\"Starting BRIGHT evaluation pipeline\", \"MAIN\")\n",
    "\n",
    "# Run the evaluation\n",
    "results_df, all_results = run_bright_benchmark()\n",
    "\n",
    "# Create visualizations\n",
    "visualize_bright_results(results_df)\n",
    "\n",
    "# Print final summary\n",
    "if not results_df.empty:\n",
    "    best_model = results_df.groupby('Model')['nDCG@10'].mean().idxmax()\n",
    "    best_score = results_df.groupby('Model')['nDCG@10'].mean().max()\n",
    "    log_with_timestamp(f\"Best performing model: {best_model} with average nDCG@10 = {best_score:.2f}%\", \"SUMMARY\")\n",
    "\n",
    "log_with_timestamp(\"BRIGHT evaluation pipeline completed\", \"MAIN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. NanoBEIR Evaluation (Optional)\n",
    "\n",
    "This section provides code to evaluate models on the NanoBEIR benchmark for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_nanobeir(model_config):\n",
    "    \"\"\"Evaluate a model on the NanoBEIR benchmark\"\"\"\n",
    "    log_with_timestamp(f\"Starting NanoBEIR evaluation for {model_config['name']}\", \"START\")\n",
    "    \n",
    "    try:\n",
    "        if model_config[\"type\"] == \"ColBERT\":\n",
    "            # For ColBERT models, we need to use PyLate\n",
    "            model = models.ColBERT(\n",
    "                model_name_or_path=model_config[\"model_id\"],\n",
    "                query_length=model_config.get(\"query_length\", 128),\n",
    "                device=model_config.get(\"device\", \"cpu\")\n",
    "            )\n",
    "            \n",
    "            # Use PyLate's NanoBEIR evaluator\n",
    "            from pylate.evaluation import NanoBEIREvaluator\n",
    "            evaluator = NanoBEIREvaluator()\n",
    "            results = evaluator(model)\n",
    "            \n",
    "        else:  # Dense models\n",
    "            # For dense models, use SentenceTransformer\n",
    "            model_kwargs = {}\n",
    "            if model_config.get(\"trust_remote_code\", False):\n",
    "                model_kwargs[\"trust_remote_code\"] = True\n",
    "                \n",
    "            model = SentenceTransformer(\n",
    "                model_config[\"model_id\"],\n",
    "                device=model_config.get(\"device\", \"cpu\"),\n",
    "                **model_kwargs\n",
    "            )\n",
    "            \n",
    "            # Use PyLate's NanoBEIR evaluator\n",
    "            from pylate.evaluation import NanoBEIREvaluator\n",
    "            evaluator = NanoBEIREvaluator()\n",
    "            results = evaluator(model)\n",
    "            \n",
    "        # Save results\n",
    "        output_dir = os.path.join(OUTPUT_DIR, \"nanobeir\")\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        result_file = os.path.join(output_dir, f\"{model_config['name']}_nanobeir_results.json\")\n",
    "        with open(result_file, 'w') as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "            \n",
    "        log_with_timestamp(f\"NanoBEIR results for {model_config['name']} saved to {result_file}\", \"SAVE\")\n",
    "        log_with_timestamp(f\"NanoBEIR nDCG@10: {results['nDCG@10']:.4f}\", \"RESULTS\")\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error in NanoBEIR evaluation for {model_config['name']}: {str(e)}\"\n",
    "        log_with_timestamp(error_msg, \"ERROR\")\n",
    "        log_with_timestamp(f\"Traceback:\\n{traceback.format_exc()}\", \"ERROR\")\n",
    "        return {\"error\": str(e), \"nDCG@10\": 0.0}\n",
    "    \n",
    "    finally:\n",
    "        # Clean up\n",
    "        if 'model' in locals():\n",
    "            del model\n",
    "        cleanup_memory()\n",
    "        log_with_timestamp(\"Memory cleanup completed\", \"CLEANUP\")\n",
    "        \n",
    "def run_nanobeir_benchmark():\n",
    "    \"\"\"Run NanoBEIR benchmark for all models\"\"\"\n",
    "    log_with_timestamp(\"Starting NanoBEIR benchmark evaluation\", \"START\")\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for model_config in MODELS_TO_TEST:\n",
    "        log_with_timestamp(f\"Evaluating {model_config['name']} on NanoBEIR\", \"MODEL\")\n",
    "        \n",
    "        try:\n",
    "            model_results = evaluate_nanobeir(model_config)\n",
    "            \n",
    "            # Extract scores\n",
    "            if \"error\" not in model_results:\n",
    "                ndcg_10 = model_results.get(\"nDCG@10\", 0.0) * 100\n",
    "            else:\n",
    "                ndcg_10 = 0.0\n",
    "                \n",
    "            results.append({\n",
    "                \"Model\": model_config[\"name\"],\n",
    "                \"Type\": model_config[\"type\"],\n",
    "                \"nDCG@10\": ndcg_10\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            log_with_timestamp(f\"Failed to evaluate {model_config['name']} on NanoBEIR: {str(e)}\", \"ERROR\")\n",
    "            results.append({\n",
    "                \"Model\": model_config[\"name\"],\n",
    "                \"Type\": model_config[\"type\"],\n",
    "                \"nDCG@10\": 0.0\n",
    "            })\n",
    "    \n",
    "    # Create DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Save results\n",
    "    results_df.to_csv(os.path.join(OUTPUT_DIR, \"nanobeir_results.csv\"), index=False)\n",
    "    \n",
    "    # Visualize\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(data=results_df, x=\"Model\", y=\"nDCG@10\", hue=\"Type\")\n",
    "    plt.title(\"NanoBEIR Benchmark Results\", fontsize=16)\n",
    "    plt.ylabel(\"nDCG@10 (%)\")\n",
    "    plt.xlabel(\"Model\")\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUTPUT_DIR, 'nanobeir_results.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\nNanoBEIR Results:\")\n",
    "    display(results_df.sort_values(\"nDCG@10\", ascending=False))\n",
    "    \n",
    "    log_with_timestamp(\"NanoBEIR evaluation completed\", \"COMPLETE\")\n",
    "    \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to run NanoBEIR evaluation\n",
    "# nanobeir_df = run_nanobeir_benchmark()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Combined Analysis (BRIGHT + NanoBEIR)\n",
    "\n",
    "This section provides code to compare results across both benchmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_analysis(bright_df, nanobeir_df):\n",
    "    \"\"\"Create combined analysis of BRIGHT and NanoBEIR results\"\"\"\n",
    "    if bright_df.empty or nanobeir_df.empty:\n",
    "        log_with_timestamp(\"Missing data for combined analysis\", \"WARNING\")\n",
    "        return\n",
    "    \n",
    "    log_with_timestamp(\"Creating combined analysis\", \"ANALYSIS\")\n",
    "    \n",
    "    # Calculate average BRIGHT scores per model\n",
    "    bright_avg = bright_df.groupby('Model')['nDCG@10'].mean().reset_index()\n",
    "    bright_avg['Benchmark'] = 'BRIGHT'\n",
    "    \n",
    "    # Prepare NanoBEIR data\n",
    "    nanobeir_avg = nanobeir_df[['Model', 'nDCG@10']].copy()\n",
    "    nanobeir_avg['Benchmark'] = 'NanoBEIR'\n",
    "    \n",
    "    # Combine data\n",
    "    combined = pd.concat([bright_avg, nanobeir_avg])\n",
    "    \n",
    "    # Create comparison plot\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    sns.barplot(data=combined, x='Model', y='nDCG@10', hue='Benchmark')\n",
    "    plt.title('Comparison of Model Performance Across Benchmarks', fontsize=16)\n",
    "    plt.ylabel('Average nDCG@10 (%)')\n",
    "    plt.xlabel('Model')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUTPUT_DIR, 'combined_benchmark_comparison.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Create summary table\n",
    "    summary = combined.pivot_table(values='nDCG@10', index='Model', columns='Benchmark')\n",
    "    summary['Overall Average'] = summary.mean(axis=1)\n",
    "    \n",
    "    print('\\nCombined Benchmark Summary:')\n",
    "    display(summary.round(2).sort_values('Overall Average', ascending=False))\n",
    "    \n",
    "    # Save summary\n",
    "    summary.to_csv(os.path.join(OUTPUT_DIR, 'combined_benchmark_summary.csv'))\n",
    "    \n",
    "    log_with_timestamp(\"Combined analysis completed\", \"COMPLETE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to run combined analysis if both benchmarks have been evaluated\n",
    "# if 'nanobeir_df' in locals():\n",
    "#     combined_analysis(results_df, nanobeir_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Conclusion\n",
    "\n",
    "This notebook has implemented the evaluation of embedding models on the BRIGHT benchmark following the original author's reference implementation. The key findings are:\n",
    "\n",
    "1. **Methodology**: We used MTEB to load BRIGHT tasks and PyLate with PLAID indexing for ColBERT models, exactly matching the reference implementation.\n",
    "\n",
    "2. **Models Evaluated**:\n",
    "   - Late-Interaction: `Reason-ModernColBERT`, `GTE-ModernColBERT-v1`\n",
    "   - Dense: `jina-embeddings-v3`, `Qwen3-Embedding-0.6B`\n",
    "\n",
    "3. **Key Results**: (See the summary tables and visualizations above)\n",
    "\n",
    "4. **Implementation Details**:\n",
    "   - Proper memory management to avoid OOM issues\n",
    "   - Comprehensive logging for debugging\n",
    "   - Robust error handling\n",
    "   - Results saved in standard formats\n",
    "   - Visualizations for easy comparison\n",
    "\n",
    "All results are saved in the `bright_evaluation_results` directory for further analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (reasoning)",
   "language": "python",
   "name": "reasoning_py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
